
# ğŸ“ˆ Linear Regression from Scratch in Python

This project demonstrates a full implementation of **Linear Regression** using only **NumPy**, written entirely from scratch â€” without relying on libraries like `scikit-learn`. It includes:

- Preprocessing and standardization
- A custom linear regression class with gradient descent
- Evaluation metrics: **MSE**, **RMSE**, and **RÂ²**
- Interactive cost plotting with Plotly

---

## ğŸ“ Project Structure

```
linear-regression/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test.csv           # Sample input dataset
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Linear_regression_from_scratch.ipynb
â”œâ”€â”€ model/
â”‚   â””â”€â”€ linear_regression.py  # The model implementation (optional extraction)
â””â”€â”€ README.md
```

---

## ğŸ” Problem Statement

We aim to fit a simple linear regression model:
```
f(x) = wx + b
```
Given input features `x` and labels `y`, the model learns the optimal parameters `w` and `b` using **gradient descent** to minimize the **Mean Squared Error (MSE)**.

---

## ğŸ§  Key Concepts

### âœ… Forward Pass
Predicts:
```
yÌ‚ = X Â· w + b
```

### âŒ Loss Function
Mean Squared Error:
```
J(w, b) = (1/2m) Î£ (yÌ‚ - y)Â²
```

### ğŸ” Backward Pass
Gradient computation:
```
âˆ‚J/âˆ‚w = (1/m) Î£ (yÌ‚ - y) * x
âˆ‚J/âˆ‚b = (1/m) Î£ (yÌ‚ - y)
```

### ğŸ”§ Parameter Update
```
w = w - Î± * âˆ‚J/âˆ‚w
b = b - Î± * âˆ‚J/âˆ‚b
```

---

## ğŸš€ Usage

### 1. Upload Your Data
Ensure `test.csv` contains columns `x` and `y`.

```bash
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test.csv
```

### 2. Run the notebook
Open and run:  
[`Linear_regression_from_scratch.ipynb`](notebooks/Linear_regression_from_scratch.ipynb)

### 3. Train the Model

```python
model = Linear_Regresion(learning_rate=0.01)
model.fit(X_train, y_train, iterations=10000)
```

### 4. Predict and Evaluate

```python
y_pred = model.predict(X_test)

mse = Mse(y_test, y_pred)
rmse = Rmse(y_test, y_pred)
r2 = R2(y_test, y_pred)
```

---

## ğŸ“Š Evaluation Metrics

| Metric | Meaning | Formula |
|--------|---------|---------|
| **MSE**  | Average squared error | ![MSE](https://latex.codecogs.com/svg.image?\text{MSE}=\frac{1}{n}\sum(y_{\text{true}}-y_{\text{pred}})^2) |
| **RMSE** | Square root of MSE | ![RMSE](https://latex.codecogs.com/svg.image?\text{RMSE}=\sqrt{\text{MSE}}) |
| **RÂ²**   | Explained variance | ![R2](https://latex.codecogs.com/svg.image?R^2=1-\frac{SSR}{SST}) |

---

## ğŸ“‰ Sample Output

- âœ… Cost convergence plot
- âœ… Model coefficients `w`, `b`
- âœ… Metrics:
  - `MSE: 0.0213`
  - `RMSE: 0.146`
  - `RÂ²: 0.97`

---

## ğŸ’¾ Model Persistence

```python
# Save model
model.save_model("linear_model.pkl")

# Load model
model.load_model("linear_model.pkl")
```

---

## ğŸ”§ Dependencies

- Python â‰¥ 3.7
- NumPy
- Pandas
- Plotly
- Google Colab (for `files.upload()`)

---

## ğŸ§ª Example Dataset Format

```csv
x,y
1.1,2.3
2.0,4.1
3.5,7.2
...
```

---

## ğŸ“š Future Improvements

- Multi-feature support
- Batch size (mini-batch gradient descent)
- Regularization (L1/L2)
- Scikit-learn benchmark comparison

---

## ğŸ¤– Author

**J.K.** â€” Machine Learning enthusiast & student of Applied Computer Science at UJ  
ğŸŒ Check out more: [YourGitHubProfile](https://github.com/yourusername)
